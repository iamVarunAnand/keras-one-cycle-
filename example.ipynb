{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.callbacks import Callback, LambdaCallback\n",
    "from keras.datasets import cifar10, mnist\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.layers.pooling import GlobalMaxPooling2D\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers import concatenate\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "from classification_models.keras import Classifiers\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config = config)\n",
    "set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneCycleScheduler(Callback):\n",
    "    def __init__(self, epochs, max_lr, steps_per_epoch, moms = (0.95, 0.85), div_factor = 25, start_pct = 0.3):\n",
    "        # initialize the instance variables\n",
    "        self.max_lr = max_lr\n",
    "        self.moms = moms\n",
    "        self.div_factor = div_factor\n",
    "        self.st1_epochs = int(np.floor(epochs * start_pct))\n",
    "        self.st2_epochs = epochs - self.st1_epochs\n",
    "        self.st1_steps = self.st1_epochs * steps_per_epoch\n",
    "        self.st2_steps = self.st2_epochs * steps_per_epoch\n",
    "        self.history = {\"lrs\" : [], \"moms\" : []}\n",
    "    \n",
    "    def __annealing_cos(self, start, end, pct):\n",
    "        \"Cosine anneal from `start` to `end` as pct goes from 0.0 to 1.0.\"\n",
    "\n",
    "        cos_out = np.cos(np.pi * pct) + 1    \n",
    "        return end + (start - end) / 2 * cos_out\n",
    "    \n",
    "    def on_train_begin(self, logs = None):\n",
    "        # initialize the necessary variables\n",
    "        self.steps_so_far = 0         \n",
    "    \n",
    "    def on_batch_begin(self, batch, logs = None):\n",
    "        # increment the step count         \n",
    "        self.steps_so_far += 1\n",
    "        \n",
    "        # check to determine the training phase\n",
    "        if self.steps_so_far <= self.st1_steps:\n",
    "            # calculate the new learning rate             \n",
    "            new_lr = self.__annealing_cos(self.max_lr / self.div_factor, \n",
    "                                          self.max_lr, \n",
    "                                          self.steps_so_far / self.st1_steps)\n",
    "            \n",
    "            # calculate the new momentum\n",
    "            new_mom = self.__annealing_cos(self.moms[0],\n",
    "                                          self.moms[1],\n",
    "                                          self.steps_so_far / self.st1_steps)\n",
    "            \n",
    "            # set the new learning rate and momentum\n",
    "            K.set_value(self.model.optimizer.lr, new_lr)\n",
    "            K.set_value(self.model.optimizer.momentum, new_mom)\n",
    "\n",
    "        else:\n",
    "            # calculate the new learning rate             \n",
    "            new_lr = self.__annealing_cos(self.max_lr, \n",
    "                                          self.max_lr / self.div_factor, \n",
    "                                          (self.steps_so_far - self.st1_steps) / self.st2_steps)\n",
    "            \n",
    "            # calculate the new momentum\n",
    "            new_mom = self.__annealing_cos(self.moms[1],\n",
    "                                           self.moms[0],\n",
    "                                           (self.steps_so_far - self.st1_steps) / self.st2_steps)\n",
    "            \n",
    "            # set the new learning rate and momentum\n",
    "            K.set_value(self.model.optimizer.lr, new_lr)\n",
    "            K.set_value(self.model.optimizer.momentum, new_mom)\n",
    "            \n",
    "        # update the history attribute\n",
    "        self.history[\"lrs\"].append(new_lr)\n",
    "        self.history[\"moms\"].append(new_mom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    @staticmethod\n",
    "    def build(base_model, classes):\n",
    "        #  GlobalConcatPooling  [AveragePooling + MaxPooling]\n",
    "        x1 = GlobalMaxPooling2D()(base_model.output)\n",
    "        x2 = GlobalAveragePooling2D()(base_model.output)\n",
    "        x = concatenate([x1, x2], axis = -1)\n",
    "        \n",
    "        # BN => DO => FC => RELU block\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(256, kernel_initializer = \"he_normal\")(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        \n",
    "        # Softmax classifier\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(classes, kernel_initializer = \"he_normal\")(x)\n",
    "        x = Activation(\"softmax\")(x)\n",
    "        \n",
    "        # return the constructed model architecture    \n",
    "        return Model(inputs = base_model.input, outputs = x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the base model\n",
    "ResNet18, preprocess_input = Classifiers.get(\"resnet18\")\n",
    "base_model = ResNet18((32, 32, 3), weights = \"imagenet\", include_top = False)\n",
    "\n",
    "# freeze the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# construct the classifier\n",
    "model = Classifier.build(base_model, 10)\n",
    "\n",
    "# compile the model\n",
    "opt = SGD(lr = 0.01, momentum = 0.9, nesterov = True)\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = opt, metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# preprocess the images\n",
    "x_train = preprocess_input(x_train)\n",
    "x_test = preprocess_input(x_test)\n",
    "\n",
    "# convert the labels from integers into vectors\n",
    "lb = LabelBinarizer()\n",
    "y_train = lb.fit_transform(y_train)\n",
    "y_test = lb.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the training parameters\n",
    "epochs = 5\n",
    "bs = 128\n",
    "steps_per_epoch = np.ceil(x_train.shape[0] / bs)\n",
    "max_lr = 0.01\n",
    "\n",
    "# initialize the one cycle scheduler\n",
    "ocs = OneCycleScheduler(epochs, max_lr, steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0403 10:22:28.226047 140187705644864 deprecation.py:323] From /home/varun/environments/pyimagesearch/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "50000/50000 [==============================] - 26s 513us/step - loss: 2.1709 - acc: 0.3233 - val_loss: 1.6559 - val_acc: 0.4294\n",
      "Epoch 2/5\n",
      "50000/50000 [==============================] - 22s 446us/step - loss: 1.5580 - acc: 0.4482 - val_loss: 1.6371 - val_acc: 0.4347\n",
      "Epoch 3/5\n",
      "50000/50000 [==============================] - 23s 469us/step - loss: 1.4987 - acc: 0.4654 - val_loss: 1.6193 - val_acc: 0.4291\n",
      "Epoch 4/5\n",
      "50000/50000 [==============================] - 23s 455us/step - loss: 1.4635 - acc: 0.4804 - val_loss: 1.6229 - val_acc: 0.4285\n",
      "Epoch 5/5\n",
      "50000/50000 [==============================] - 22s 447us/step - loss: 1.4489 - acc: 0.4869 - val_loss: 1.6129 - val_acc: 0.4280\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "H = model.fit(x_train, y_train, validation_data = (x_test, y_test), \n",
    "          epochs = epochs, batch_size = bs,\n",
    "          callbacks = [ocs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize = (8, 3))\n",
    "ax[0].plot(ocs.history[\"lrs\"])\n",
    "ax[0].set_title(\"Learning Rate\")\n",
    "ax[0].set_xlabel(\"Iterations\")\n",
    "ax[1].plot(ocs.history[\"moms\"])\n",
    "ax[1].set_title(\"Momentum\")\n",
    "ax[1].set_xlabel(\"Iterations\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python36964bit0a6a3af0c6c942d9aaac98da71ec5a43"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
